---
title             : "The benefits of reporting critical effect size values"
shorttitle        : "Reporting critical effect size values"
author: 
  - name          : "Ambra Perugini"
    affiliation   : "1"
    role:         # Contributorship roles (e.g., CRediT, https://casrai.org/credit/)
      - "Conceptualization"
      - "Writing - Original Draft Preparation"
      - "Writing - Review & Editing"
      - "Methodology"
      - "Software"
  - name          : "Filippo Gambarota"
    affiliation   : "1"
    role:
      - "Writing – Original Draft Preparation"
      - "Writing - Review & Editing"
      - "Methodology"
      - "Software"
  - name          : "Enrico Toffalini"
    affiliation   : "2"
    role:
      - "Writing – Original Draft Preparation"
      - "Writing - Review & Editing"
      - "Software"
      - "Supervision"
  - name          : "Daniël Lakens"
    affiliation   : "3"
    role:
      - "Conceptualization"
      - "Writing - Review & Editing"
      - "Supervision"
  - name          : "Massimiliano Pastore"
    affiliation   : "1"
    role:
      - "Conceptualization"
      - "Writing - Review & Editing"
      - "Supervision"
  - name          : "Livio Finos"
    affiliation   : "4"
    role:
      - "Conceptualization"
      - "Writing - Review & Editing"
      - "Supervision"
  - name          : "Psicostat"
    affiliation   : "1"
    role:
      - "Conceptualization"
      - "Writing - Review & Editing"
      - "Supervision"
  - name          : "Gianmarco Altoè"
    affiliation   : "1"
    corresponding: "yes"
    address       : "Via Venezia 8, 35131, Padova, Italy"
    email         : "gianmarco.altoe@unipd.it"
    role:
      - "Conceptualization"
      - "Writing - Review & Editing"
      - "Supervision"
affiliation:
  - id            : "1"
    institution   : "Department of Developmental and Social Psychology, University of Padova, Italy"
  - id            : "2"
    institution   : "Department of General Psychology, University of Padova, Italy"
  - id: "3"
    institution: "Eindhoven University of Technology, Netherlands"
  - id: "4"
    institution: "Department of Statistics, University of Padova, Italy"

abstract: |
  Critical effect size values represent the smallest detectable effect that would reach statistical significance given a certain sample size. These values are highly applicable in various scenarios, as they do not require specifying a plausible effect size of interest. Reporting critical effect size values may be especially useful when adopting NHST (Null Hypothesis Significance Testing) and the sample size has not been planned a priori, or when there is uncertainty about the expected effect size. To assist researchers in calculating critical effect size values, we have developed an R package that allows to report the critical effect size values of model parameters. The package's functions are implemented for group comparisons, correlations, linear regression, and meta-analysis. What we propose could benefit researchers during the planning phase of the study by helping them to understand the limitations and strengths of their research design, and also as a retrospective tool to potentially reframe original interpretations. 

keywords          : critical effect size values, statistical significance, fucntions implementation 
wordcount         : "3912"

bibliography      : "bibliography.bib"
csl               : "`r system.file('rmd', 'apa7.csl', package = 'papaja')`"
documentclass     : "apa7"
floatsintext      : no
linenumbers       : no
draft             : no
mask              : no

figurelist        : no
tablelist         : no
footnotelist      : no

classoption       : "man"
output            : 
    papaja::apa6_pdf:
      latex_engine: xelatex
header-includes:
  - |
    \makeatletter
    \renewcommand{\paragraph}{\@startsection{paragraph}{4}{\parindent}%
      {0\baselineskip \@plus 0.2ex \@minus 0.2ex}%
      {-1em}%
      {\normalfont\normalsize\bfseries\typesectitle}}
    
    \renewcommand{\subparagraph}[1]{\@startsection{subparagraph}{5}{1em}%
      {0\baselineskip \@plus 0.2ex \@minus 0.2ex}%
      {-\z@\relax}%
      {\normalfont\normalsize\bfseries\itshape\hspace{\parindent}{#1}\textit{\addperi}}{\relax}}
    \makeatother
    \usepackage{xcolor}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      comment = "#>",
                      size = "footnotesize")

def.chunk.hook  <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
  x <- def.chunk.hook(x, options)
  ifelse(options$size != "normalsize", paste0("\n \\", options$size,"\n\n", x, "\n\n \\normalsize"), x)
})
```


In the present paper, we suggest that researchers should systematically report critical effect size values when they make inference using statistical significance. The "critical effect size value(s)" refers to the smallest statistically significant effect sizes, which depend on the test performed and the characteristics of the sample [@lakens2022sample]. To cut to the chase, let us consider a study that tests (using for example a two-tailed test) a bivariate correlation with $n = 20$ participants and uses $p < 0.05$ as the threshold for statistical significance: in this case, the smallest significant effects (critical effect size values) are Pearson's $r = -0.44$ and $r = 0.44$. As we will argue, critical effect size values can serve as a useful complement to aid in the interpretation of study results, to communicate their relevance, and to help assess their replicability, particularly when statistical significance is relied upon for inference. This is especially important in cases where the sample size was not (or could not) be predetermined based on a theory-driven target effect size, or when statistical power may be difficult to determine or deviates from an optimal level. 

Despite longstanding criticisms, Null Hypothesis Significance Testing (NHST) remains a prominent approach for statistical inference in science [@cohen1994; @gigerenzer2004null; @krueger2001null]. Over the past decades, numerous proposals have emerged to enhance inference based on this approach. These include conducting power analyses to determine sample size \textit{a priori} based on theory-informed target effects, and reporting effect sizes and their confidence intervals, accompanied by pertinent related comments, rather than drawing dichotomous conclusions solely based on statistical significance [@transue2019apa]. Even for the most skeptical, there are suggestions that NHST may possess merits of its own. For instance, @wilson2020science propose that it could serve as a useful preliminary filter utilized by "original science" for screening potentially interesting effects, which should subsequently be validated by "replication science". 

In psychology, statistical power is often severely limited for medium to small effect sizes [@szucs2017empirical], which are generally expected to represent true replicable findings. In fact, resource constraints have been authoritatively listed among acceptable justifications for sample size  [@lakens2022sample], and this scenario is arguably widespread in psychology. However, limited power makes it challenging to distinguish signal from noise, and on average it results in an overestimation of effect sizes when relying on statistical significance for inference [@altoe2020enhancing]. On the other hand, very large samples are occasionally available in psychological science, albeit less frequently. However, as some have argued, excessively large sample sizes risk creating an "everything-is-significant" scenario, where researchers report and discuss practically negligible effects solely because they are statistically significant, regardless of theoretical relevance or whether they merely reflect minor procedural artifacts [@wilson2020science]. Thus, while statistical significance can serve as a potentially useful filter, relying solely on it for inference may introduce risks in interpretation. In such cases, we contend that reporting critical effect size values can help put into perspective and convey the relevance of the results.

There are two clear benefits to reporting the critical effect size value(s) for a corresponding test. First, when sample sizes are small, the critical effect size values inform readers about whether the effect sizes that could lead to rejecting the null hypothesis are in line with realistic expectations. If the sample size is small and only very large effects would yield a statistically significant result, and the underlying mechanism that is examined is unlikely to lead to such large effect sizes, in these cases, researchers will realize they are not able to collect sufficient data to perform a meaningful test. An a-priori power analysis would typically lead to a similar conclusion, but reporting critical effect size values focuses the attention more strongly on which effect sizes are reasonable to expect. Second, in large studies the critical effect size values will make it clear that trivially small effect sizes will be statistically significant. This will focus the attention of researchers on the difference between statistical significance and practical significance. This is especially important in correlational studies on large samples in psychology, where small uncontrolled sources of statistical noise may simultaneously affect and create some shared variance across otherwise unrelated variables, a phenomenon controversially labelled as the 'crud factor' [@orben_crud_2020].

Let us consider the following two scenarios as examples. First, imagine researchers conducting a study involving a between-group comparison. Due to severe resource constraints, they are only able to collect a limited sample size of $n = 30$ ($n = 15$ per group). No statistically significant effect is detected with $p < 0.05$. Given their prior expectation that the effect of interest may not be large, they acknowledge that their study was likely underpowered, though uncertain to some extent. Subsequently, they compute the critical effect size values, revealing Cohen's $d = -0.75$ and $d = 0.75$. This indicates that any estimated |Cohen's $d| < 0.75$ will certainly fail to reach statistical significance. By reporting these critical effect size values, the researchers transparently convey that estimated effects up to a medium-to-large magnitude will consistently fall short of significance. In essence, this scenario exemplifies a version of the "winner's curse" [@hedges1984estimation], made explicit. In this context, the winner's curse indicates the tendency for a statistically significant initial finding (i.e., a "winner") to be associated with an overestimated effect size which will likely be deflated in subsequent replications (i.e., the "curse"), especially if such initial finding was obtained with a small sample. Knowing from the critical effect size values that a specific result must necessarily be associated with either an estimated large effect size or be non-significant, readers are, in principle, prevented from both over-interpreting the magnitude of an effect size and from equating lack of statistical significance with lack of an effect of interest. 

In a second scenario, imagine researchers who gain access to a very large archival dataset ($n = 5,000$) and decide to explore some bivariate correlations. With such a large sample size, researchers are not constrained by sample size limitations and could instead focus on effect sizes. However, they legitimately  opt to use statistical significance of a two-tailed test as their primary criterion for whether to comment on effects in their further discussions. In this context, the critical effect size values for significance are determined to be correlations of $r = \pm 0.03$. This implies that practically any effect differing from zero, regardless of its small magnitude, may easily attain statistical significance. It is worth noting that such small effects may potentially reflect just minor artifacts [@wilson2020science], such as subtle experimenter effects or slight non-independence among observations, even in meticulously designed studies. While readers may be tempted to interpret any reported effects simply because they attain statistical significance, signaling the critical effect size value level beforehand can serve as a clear warning that the significance filter might be overly permissive, thus urging interpretive caution. 

A similar scenario may arise in meta-analysis. Despite potential loss of precision due to substantial heterogeneity across effect sizes in different studies, meta-analyses typically synthesize a large amount of evidence, leading even very small average effect sizes to reach significance. While the focus of meta-analysis is generally on estimating effect sizes with uncertainty, statistical significance is routinely reported and interpreted. Thus, in meta-analysis, reporting the critical effect size values can caution readers that the mere attainment of statistical significance by the average effect does not automatically ensure scientific or real-life relevance. 

To provide more tangible illustrations of the practical applications of critical effect size values, let us examine two real-world instances drawn from published research. The first one features a small sample and has been targeted in the series of replications by the Open Science Collaboration [@open2015estimating], while the second one presents with a very large sample and has already attracted attention in the scientific community. The first case pertains to "Study 5" conducted by @mccrea2008self. This experiment compared two groups based on their performance percentage in a math test following a preliminary practice session with feedback of failure. One group was exposed to failure-excusing, self-handicapping thoughts, while both groups underwent the math test afterward. With a modest sample size of 28 participants split into two groups (13 and 15 participants, respectively), a directional one-tailed t-test was conducted on the percentage of correct answers to compare the groups. The resulting effect was statistically significant, $t(26) = 1.87$, $p$ (one-tailed) $< .05$. Based on the reported data, Cohen's $d = 0.736$, indicating a considerable effect size. However, the critical value for Cohen's $d$ in this context was $0.645$, suggesting that, given this sample size, statistical significance is achieved only for estimated effects of substantial magnitude. Subsequent replication efforts [@open2015estimating] reaffirmed the significance of the effect, and used a somewhat larger sample ($n = 61$), $t(59)=2.325$. The estimated effect size was somehow lower than that originally reported, however, $d = 0.605$ (here, critical d = 0.428). The second scenario involves the study by @kramer2014experimental, which explored the impact of emotional content on Facebook users' experiences. With a notably large sample size of $n = 689'003$, the researchers observed a significant effect worsened emotional states when positive posts were reduced, $t(310,044) = −5.63$, $p < 0.001$ (non-directional test). Based on this and other results, the authors conclude that emotions expressed by other users on Facebook influence our own emotions. Despite the statistical significance, however, the critical values for Cohen's $d$ in this case are $-0.006$ and $+0.006$, emphasizing the importance of considering effect size beyond mere significance. This prompts reflection on whether an effect of such magnitude on such a large sample (actual $d = 0.02$) is truly meaningful in individuals' lives.

# How to Compute Critical Effect Size Values

In this section, we provide guidance and formulas for computing critical effect size values in general, with examples for frequently encountered effect sizes including Standardized Mean Differences (Cohen's $d$), correlations (Pearson's r), and raw and standardized coefficients in linear models. These formulas have been incorporated into R functions of the package `criticalESvalue` accessible at: [https://github.com/psicostat/criticalESvalue](https://github.com/psicostat/criticalESvalue), and are elaborated upon in the subsequent section.

## t-test

For the t-test we considered the two-sample, paired and one-sample tests. As a general approach the $t$ statistics is computed as reported in Equation \@ref(eq:eq-gen-t).

\begin{equation}
    (\#eq:eq-gen-t)
    t = \frac{b}{\text{SE}_{b}}
\end{equation}

Where $b$ is the unstandardized effect size that depends on the type of test. For example, in the two sample t-test or the single mean for a one sample t-test. The denominator is the standard error of the numerator that depends on the sample size and the sample(s) variance.

Similarly, Equation \@ref(eq:eq-gen-d) formalize a general form of the effect size.

\begin{equation}
    (\#eq:eq-gen-d)
    d = \frac{b}{s}
\end{equation}

Where $b$ is still the unstandardized effect size and $s$ is the standardization term. For example, in the two-sample case, $s$ is the pooled standard deviation between the two samples or the standard deviation of the differences for paired samples.


The critical $t_c$ is the test statistic associated with a p-value equal or lower to $\alpha$. Substituting $t_c$ into Equation \@ref(eq:eq-gen-t) we realize that there is a $b_c$ that with a certain $SE_b$ produce the critical test statistics. The general form of the critical effect size $d_c$ is defined in Equation \@ref(eq:eq-gen-crit). The $b_c$ is derived solving Equation \@ref(eq:eq-gen-t) for $b$.


\begin{equation}
\begin{gathered}
    (\#eq:eq-gen-crit)
    d_c = \frac{b_c}{s} \\
    b_c = t_c \times SE_b
\end{gathered}
\end{equation}

NOT SURE TO KEEP THIS!

Another way of conceptualizing the critical effect size is by removing the sample size from the $t$ statistics (see Equation \@ref(eq:eq-gen-t)). In fact, $SE_b$ can be generally defined as $\frac{s}{\sqrt{N}}$ where $s$ is the denominator in Equation \@ref(eq:eq-gen-d) and $n$ is the sample size. Thus, $d_c = t_c \times \frac{1}{\sqrt{n}}$ ($\frac{1}{\sqrt{n}}$ because the standard deviation of a standardized effect size is one by definition) is equivalent to \@ref(eq:eq-gen-crit). The only caveat is that calculating $d$ or $d_c$ in this way assume that the standard deviation used to calculate the test statistics and the effect size is the same. While this is true most of the time, as explained in the paired t-test section, there are more than one methods to calculate the (critical) effect size using different standard deviations. Furthemore, this method assume that the two groups (in case of a two-sample t-test) have the same size.






### One sample t-test

The easiest case is the one-sample t-test. $b$ is the sample mean and $s$ is the sample standard deviation and $n$ is the sample size. $SE_b = \frac{s}{\sqrt{n}}$ and the $t_c$ is based on $n - 1$ degrees of freedom. Thus $b_c = t_c \times SE_b$ and $d_c = \frac{b_c}{s}$. Equivalently, $d_c = t_c \times \frac{1}{\sqrt{n}}$



### Two-sample t-test

We can apply the one-sample approach to the two-sample case. When assuming homogeneity of the variances between the two groups we have $n_1 + n_2 - 2$ degrees of freedom to calculate $t_c$ and $s$ is the pooled standard deviation ($s_p$) calculated using Equation \@ref(eq:eq-s-p-twsmp). The $s_p$ can be intepreted as the square root of the weighted average of the samples standard deviations. The effect size $d$ is calculated as $\frac{b}{s_p}$ and the critical effect size can be calculated as $d_c = \frac{b_c}{s_p}$ with $b_c = t_c \times SE_b$. $SE_b$ is calculated as $s_p \sqrt{\frac{1}{n_1} + \frac{1}{n_2}}$. As for the one-sample case, we can calculate the critical effect size from the t statistics as $d_c = t_c \times \sqrt{\frac{1}{n_1} + \frac{1}{n_2}}$.

\begin{equation}
    (\#eq:eq-s-p-twsmp)
    s_p = \sqrt{\frac{s_1^2 (n_1 - 1) + s_2^2 (n_2 - 1)}{n_1 + n_2 - 2}}
\end{equation}

When relaxing the assumption of equal variances (i.e., using the Welch's t-test) $s$ is calculated as the square root of the average between the two samples variances, $SE_b$ is calculated as $\sqrt{\frac{s^2_{x_1}}{n_1} + \frac{s^2_{x_2}}{n_2}}$. The degress of freedom for $t_c$ are calculated in a more complex way using the Welch-Satterthwaite equation [@Welch1947-wh; @Satterthwaite1946-aq]. For the Welch's t-test we can calculate the critical effect size removing the sample size from $t_c$ (as done above) only when $n_1 = n_2$.

### Paired sample t-test

For the paired sample case the situation is less straightforward. The reason is that the $t$ statistic is computed using $b$ being the average of the paired differences between the two (paired) samples and $s$ being the standard deviation of the differences. Using these values, we can just compute the critical effects size as for the one-sample case, in fact a paired t-test is a one-sample test on the vector of paired differences. Test statistics and effect size based on the standard deviation of differences (the so called $d_z$) are used for hypothesis testing and power analysis [@lakens2013calculating]. The problem is that the $d_z$ cannot be directly compared to the effect size calculated using the pooled standard deviation [@morris2002combining]. If the correlation between the two paired samples is known there is a direct relationship between the pooled standard deviation $s_p$ and the standard deviation of differences $s_D$ as reported in Equation \@ref(eq:eq-sp-sd)

\begin{equation}
    (\#eq:eq-sp-sd)
    \begin{gathered}
        s_p =  \frac{s_D}{\sqrt{2(1 - \rho)}} \\
        s_D =  s_p \sqrt{2(1 - \rho)}
    \end{gathered}
\end{equation}

Even if the hypothesis testing is computed using $s_D$, we reported the critical effect size value both using the pooled standard deviation and the standard deviation of the differences.

### Hedges's correction

The effect size calculated as in the previous step is known to be inflated especially for small samples. For this reason, there is a corrected version of the effect size called Hedges's $g$ [@Hedges1981-za, @Viechtbauer2007-tq]. The Hedges's $g$ can be calculated for all the t-test scenarios. The Hegdes's correction is implemented in Equation \@ref(eq:eq-g-correction) where $\Gamma$ is the gamma function and $m$ the degrees of freedom.

\begin{equation}
(\#eq:eq-g-correction)
c(m) = \frac{\Gamma\left(\frac{m}{2}\right)}{\sqrt{\frac{m}{2}} \Gamma\left(\frac{m-1}{2}\right)} \approx 1 - \frac{3}{4m - 1}
\end{equation}

The correction is applied as $g = c(m) \times d$. The hypothesis testing is not taking into account the Hedges's correction thus the critical $g$ will be different from the critical $d$.

## Correlation Test

Hypothesis testing for the Pearson's correlation coefficient is usually done using the t statistics or using a z statistics. The general approach presented for the t-test is still valid. The only difference is that the correlation is already a standardized effect size thus there is no need of the standardization term $s$. Equation \@ref(eq:eq-gen-t) can be used substituing $b$ with $r$ (the sample correlation coefficient) as shown in Equation \@ref(eq:eq-rc). The standard error is calculated as $SE_r = \sqrt{\frac{1 - r^2}{n - 2}}$ [@Bowley1928-yt]. 

\begin{equation}
(\#eq:eq-rc)
    \begin{gathered}
        t = \frac{r}{SE_r} = r \sqrt{\frac{n - 2}{1 - r^2}} \\
        r_c = \frac{t_c}{\sqrt{n - 2 + t_c^2}}
    \end{gathered}
\end{equation}

Another approach for hypothesis testing of the Pearson's correlation coefficient is using the Fisher's $z$ transformation [@Fisher1921-qz; @Fisher1915-ks] reported in Equation \@ref(eq:eq-z-transf) and a $z$ test statistics.

\begin{equation}
    (\#eq:eq-z-transf)
    \begin{gathered}
    r_z = \frac{1}{2} \ln \left(\frac{1 + r}{1 - r} \right) =                   \text{artanh}(r) \\
    r = \frac{\exp(2r_z - 1)}{\exp(2r_z + 1)} = \text{tanh}(r_z)
    \end{gathered}
\end{equation}

We can still use Equation \@ref(eq:eq-gen-t) substituting $b$ with $r_z$. The standard error of the $r_z$ correlation is calculated as $SE_{r_z} = \frac{1}{\sqrt{n - 3}}$. Equation \@ref(eq:eq-f-rc) shows how to calculate the critical $r_{z_c}$ value where $z_c$ is the critical value of the standard normal distribution with a certain $\alpha$ level. Finally, $r_{z_c}$ can be transformed back from the Fisher's $z$ transformation using Equation \@ref(eq:eq-z-transf)

\begin{equation}
    (\#eq:eq-f-rc)
    \begin{gathered}
        z = \frac{r_z}{SE_z} = r_z \sqrt{n - 3} \\
        r_{z_c} = \frac{z_c}{\sqrt{n - 3}}
    \end{gathered}
\end{equation}

## Linear Regression

Hypothesis testing on regression coefficients (e.g., using the \texttt{lm} function in R) is performed using the so-called Wald test. The test can be considered a one-sample t-test where the $t$ value is calculated dividing the regression parameter $\beta_0, \dots, \beta_j$ by its standard error $SE_{\beta_j}$. The critical $t$ value is calculated from a $t$ distribution using $n - p - 1$ degrees of freedom where $n$ is the number of observations and $p$ is the number of coefficients beyond the intercept. We can substitute $b$ with $\beta_j$ and $SE_{b}$ with $SE_{\beta_j}$ in Equation \@ref(eq:eq-gen-crit) to find the critical unstandardized regression coefficient ($\beta_{c_j} = t_c \times SE_{\beta_j}$). There is no an unique way to calculate the standardized version of the critical regression coefficient. The main reason is that in regression modelling we can include categorical and numerical independent variables with main effects and interactions. For example, @Gross2023-fl proposed a way to calculate a generalized Cohen's $d$ from linear regression with or without the presence of other variables dividing the estimated parameter with the residual standard deviation. @Gelman2008-zp proposed to standardize using two standard deviations because when both numeric and categorical variables are included the coefficients are not on the same metric. Another complication is that in some cases, researchers standardize the dependent variable, the independent variables or both. In general, also the unstandardized critical regression coefficient can be usually easily intepreted. For example, assuming we are regressing some scores on a cognitive tests using the age of participant. The critical regression coefficient is the minimum increase in the cognitive test for a unit age increase that would be significant. 

## Meta-analysis

Meta-analysis allows to pool information from multiple studies related to a specific research question. The main advantage of meta-analysis is pooling multiple studies to obtain a more precise and powerful estimation of the effect. From a statistical point of view, a meta-analysis can be considered as a weighted linear regression with heterogeneous variances. Similarly to standard linear regression, hypothesis testing is performed using Wald $t$ or $z$ tests [@Borenstein2009-mo]. For this reason, we can apply the same equations used for the linear regression. The main difference and advantage is that the meta-analysis model usually works directly with standardized effect size measures and regression coefficients are already standardized.

## Other models

Despite we discussed only linear models, the same approach could be applied to other types of models such as generalized linear models. In fact, we simply need to multiply the critical value of the chosen distribution (e.g., $t$ or $z$) by the standard error of the regression coefficient. For example, in a logistic regression with a binary predictor the estimated regression coefficient is the (log) odds ratio. We can obtain the critical odds ratio by multiplying the critical $z$ statistics (the default in R with the `glm` function) with the standard error of the regression coefficient.
    
# Examples in R

In this section, we introduce a user-friendly implementation of the aforementioned mathematical computations as functions of the package "criticalESvalue" in R. The complete package can be accessed at: https://github.com/psicostat/criticalESvalue. Here, we demonstrate its application through two examples: one example of a t-test on real data and a computation of the critical effect size values for a correlation from sample size. It is important to note that, in general, depending on the researcher's hypotheses, the package allows for the calculation of either two critical effect size values for two-tailed tests or a single critical effect size value for one-tailed tests, for which the direction must be specified. In the Supplementary online materials additional examples can be found for correlation, t-test, paired t-test, linear models and regression coefficients.

First the package should be downloaded and opened with the library function:

```{r, download-package}
# require(remotes)
# remotes::install_github("psicostat/criticalESvalue")
library(criticalESvalue)
```

For our examples on real data we used from the package 'psych' the dataset "holzinger.swineford" which has a series of demographics and scores of different subtests measuring intelligence on 301 subjects. Once the package is retrieved with 'library', the dataset can be opened using 'data("name of the dataset")'. For simplicity we decided to rename it with a shorter name.

```{r load-dataset}
library(psych)
library(psychTools)
data("holzinger.swineford")
Holz <- holzinger.swineford
```

We want to know the critical value for a t-test comparing boys and girls on a cognitive variable of visual perception. In this case, it can be easily done using the 't.test' function:

```{r t-test}
tt <- t.test(Holz$t01_visperc[Holz$female == 1], 
             Holz$t01_visperc[Holz$female == 2], var.equal = T)
critical(tt)
```

The output gives a wide range of values: the Cohen's $d$ calculated on the data ($d$), the critical Cohen's $d$ ($dc$), the numerator of the formula for the critical Cohen's $d$ ($bc$), the Cohen's $d$ adjusted for small samples ($g$) and the critical Cohen's $d$ adjusted for small samples ($gc$). The variance is set to be equal, but if that is not the case of your data, you can also run Welch two sample $t$-test and obtain the critical effect size values for that.

In the next example we will show the use of the package's function `critical_cor` to calculate the critical effect size values for a correlation in a prospective framework.

```{r correlation}
n <- 60
critical_cor(n = n, hypothesis = "two.sided", test = "z")
```

The direction of the hypothesis and the test to apply, either $t$-test or $z$-test, should be specified. The output will return the critical correlation value(s), the degrees of freedom and the type of test used.

# Discussion

With the present article, we propose that researchers compute and report the "critical effect size value(s)" in their empirical articles. This is not intended to replace other strategies aimed at enhancing the NHST approach to inference. Such strategies, such as the emphasis on estimating effect sizes with confidence intervals [@transue2019apa] or the \textit{a priori} planning for statistical power are valuable in their own right. Instead, our proposal serves as a complementary tool, especially beneficial for facilitating the interpretation of results when statistical power deviates from an optimal level (typically falling below, but occasionally exceeding it). Interestingly, critical effect size values can be retrospectively applied even to already published studies. This possible application facilitates potential reframing of the original interpretations. Serving as a tool for retrospective analysis, critical effect size values may enable a reconsideration of the relevance of previously reported findings.

An advantage of reporting critical effect size values is that they can be precisely computed in any scenario, without requiring assumptions about the expected effect size, as is the case with power calculations. The critical effect size value represents a directly interpretable benchmark that is especially useful in situations where statistical power is below the desired level and researchers are left otherwise uncertain about how to proceed with the interpretation of a study findings. For example, let us say that we read a published article reporting some effects as statistically significant, while others as not: we suspect that the study may be underpowered, but we are widely uncertain about the magnitude of possible true effects. To what extent can the reported results be interpreted, precisely? Knowing the critical effect size value provides us with a clear benchmark. Conversely, let us say that an effect achieves significance in a very large sample: researchers tend to draw substantive conclusions based on this. But is it of real theoretical relevance? If in comparing two groups, such as controls versus treatment, any Cohen's $d > 0.07$ would reach significance, is statistical significance enough to signal a "successful" treatment? Maybe yes, even if effects are tiny [e.g., @funder2019evaluating], but knowing the critical effect size value certainly prompts some appropriate interpretive caution.

Reporting the critical effect size value(s) can also be an efficient way to allow researchers to evaluate which findings are statistically significant. For example, in a correlation table researchers customarily add an asterisk to all statistically significant correlations. But as long as all correlations are based on the same sample size, researchers can simply remark 'the critical effect size is $r = 0.3$' and readers will know that all correlations larger than this value are statistically significant.

Beyond enhancing study design and statistical inferences based on hypothesis tests, reporting critical effect size values can also serve an educational purpose. It underscores how the distinction between a significant and non-significant result is not solely determined by the presence or absence of a true effect, but also by the sample size. By highlighting a critical effect size value, researchers can become more aware of the possibility of Type 2 errors when results are non-significant. Conversely, in studies with exceedingly large samples and in many meta-analyses, the critical effect size value(s) may serve as a reminder that any observed effect larger than a trivially small value will likely achieve significance. This emphasizes that the mere attainment of statistical significance in a test is not particularly surprising, especially in non-experimental studies.

Real-case scenarios may not always be that simple. Hence, we chose to expand the application of computing critical effect size values beyond Cohen's $d$ and correlation to include linear regression with both raw and standardized coefficients. This serves as a first step in computing critical effect size values for a wider array of effects encountered in practical scenarios, where linear models and their extensions are commonly utilized for modeling purposes. A prerequisite is that researchers must be able to identify what parameters in their statistical models reflect the effect sizes of interest, and that they are able to assess their relevance. Notably, however, this prerequisite aligns with the requirements of APA style guidelines concerning the reporting of effect sizes. For further illustration and application, additional examples are provided in the Supplementary online material. 

We suggest that reporting critical effect size values is particularly valuable when sample size planning was not feasible or did not occur \textit{a priori}. In cases where optimal power can be attained with a sufficiently large sample size for an effect of a specific magnitude of interest, and this is truly determined \textit{a priori}, the interpretation of both significance and non-significance becomes straightforward. However, when power analysis did not inform the sample size or when power is likely but undeterminedly low, reporting critical effect size values for the obtained sample can help provide context for interpretation. Critical effect size values can be computed and interpreted even retrospectively or for studies that have already been published. 

In conclusion, reporting critical effect size values in empirical articles serves as a valuable addition to researchers' toolkit, aimed at augmenting transparency and facilitating the interpretability of their findings. While not designed to supplant existing practices, it provides a useful aid in interpreting newly presented and previously published results, thus advancing the understanding of research outcomes. 

\newpage

# References

::: {#refs custom-style="Bibliography"}
:::
